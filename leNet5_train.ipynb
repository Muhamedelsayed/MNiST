{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5 Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "from evaluation_matrix import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.LayerObjects import *\n",
    "from utils.utils_func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the path of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r'./MNIST/t10k-images-idx3-ubyte'\n",
    "test_label_path = r'./MNIST/t10k-labels-idx1-ubyte'\n",
    "train_image_path = r'./MNIST/train-images-idx3-ubyte'\n",
    "train_label_path = r'./MNIST/train-labels-idx1-ubyte'\n",
    "trainset = (train_image_path, train_label_path)\n",
    "testset = (test_image_path, test_label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the dataset with readDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_image, train_label) = readDataset(trainset)\n",
    "(test_image, test_label) = readDataset(testset)\n",
    "n_m, n_m_test = len(train_label), len(test_label)\n",
    "print(\"The shape of training image:\", train_image.shape)\n",
    "print(\"The shape of testing image: \", test_image.shape)\n",
    "print(\"Length of the training set: \", n_m)\n",
    "print(\"Length of the training set: \", n_m_test)\n",
    "print(\"Shape of a single image: \", train_image[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Zero-padding & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_normalized_pad = normalize(zero_pad(train_image[:,:,:,np.newaxis], 2),'lenet5')\n",
    "test_image_normalized_pad  = normalize(zero_pad(test_image[:,:,:,np.newaxis],  2),'lenet5')\n",
    "print(\"The shape of training image with padding:\", train_image_normalized_pad.shape)\n",
    "print(\"The shape of testing image with padding: \", test_image_normalized_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Structure of LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers used here is:\n",
    "**C1** → a1 → **S2** → **C3** → a2 → **S4** → **C5** → a3 → **F6** → a4 → **RBF**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C3_mapping = [[0,1,2],[1,2,3],[2,3,4],[3,4,5],[4,5,0],[5,0,1],              [0,1,2,3],[1,2,3,4],[2,3,4,5],[3,4,5,0],[4,5,0,1],[5,0,1,2],              [0,1,3,4],[1,2,4,5],[0,2,3,5],              [0,1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[63]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixed weight (7x12 preset ASCII bitmaps) used in the RBF layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitmap = rbf_init_weight()\n",
    "fig, axarr = plt.subplots(2,5,figsize=(20,8))\n",
    "for i in range(10):\n",
    "    x,y = int(i/5), i%5\n",
    "    axarr[x,y].set_title(str(i))\n",
    "    axarr[x,y].imshow(bitmap[i,:].reshape(12,7), cmap=mpl.cm.Greys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5 object (also stored in utils/LayerObjects.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(object):\n",
    "    def __init__(self):\n",
    "        kernel_shape = {\"C1\": (5,5,1,6),\n",
    "                        \"C3\": (5,5,6,16),    ### C3 has designated combinations\n",
    "                        \"C5\": (5,5,16,120),  ### It's actually a FC layer\n",
    "                        \"F6\": (120,84),\n",
    "                        \"OUTPUT\": (84,10)}\n",
    "        \n",
    "        hparameters_convlayer = {\"stride\": 1, \"pad\": 0}\n",
    "        hparameters_pooling   = {\"stride\": 2, \"f\": 2}        \n",
    "        \n",
    "        self.C1 = ConvLayer(kernel_shape[\"C1\"], hparameters_convlayer)\n",
    "        self.a1 = Activation(\"LeNet5_squash\") # squash used to normalize vectors rather than the scalers to be used in capsule network to make a relation between same objects like nose mouth etc....\n",
    "        self.S2 = PoolingLayer(hparameters_pooling, \"average\")\n",
    "        \n",
    "        self.C3 = ConvLayer_maps(kernel_shape[\"C3\"], hparameters_convlayer, C3_mapping)\n",
    "        self.a2 = Activation(\"LeNet5_squash\")\n",
    "        self.S4 = PoolingLayer(hparameters_pooling, \"average\")\n",
    "        \n",
    "        self.C5 = ConvLayer(kernel_shape[\"C5\"], hparameters_convlayer)\n",
    "        self.a3 = Activation(\"LeNet5_squash\")\n",
    "        self.F6 = FCLayer(kernel_shape[\"F6\"])\n",
    "        self.a4 = Activation(\"LeNet5_squash\")\n",
    "        \n",
    "        self.Output = RBFLayer(bitmap)\n",
    "        \n",
    "    def Forward_Propagation(self, input_image, input_label, mode): \n",
    "        self.label = input_label\n",
    "        self.C1_FP = self.C1.foward_prop(input_image)\n",
    "        self.a1_FP = self.a1.foward_prop(self.C1_FP)\n",
    "        self.S2_FP = self.S2.foward_prop(self.a1_FP)\n",
    "        self.C3_FP = self.C3.foward_prop(self.S2_FP)\n",
    "        self.a2_FP = self.a2.foward_prop(self.C3_FP)\n",
    "        self.S4_FP = self.S4.foward_prop(self.a2_FP)\n",
    "        self.C5_FP = self.C5.foward_prop(self.S4_FP)\n",
    "        self.a3_FP = self.a3.foward_prop(self.C5_FP)\n",
    "        self.flatten = self.a3_FP[:,0,0,:]\n",
    "        self.F6_FP = self.F6.foward_prop(self.flatten)\n",
    "        self.a4_FP = self.a4.foward_prop(self.F6_FP)  \n",
    "        \n",
    "        # output sum of the loss over mini-batch when mode = 'train'\n",
    "        # output tuple of (0/1 error, class_predict) when mode = 'test'\n",
    "        out  = self.Output.foward_prop(self.a4_FP, input_label, mode) \n",
    "        return out \n",
    "        \n",
    "    def Back_Propagation(self, momentum, weight_decay):\n",
    "        dy_pred = self.Output.back_prop()\n",
    "        \n",
    "        dy_pred = self.a4.back_prop(dy_pred)\n",
    "        F6_BP = self.F6.back_prop(dy_pred, momentum, weight_decay)\n",
    "        reverse_flatten = F6_BP[:,np.newaxis,np.newaxis,:]\n",
    "        \n",
    "        reverse_flatten = self.a3.back_prop(reverse_flatten) \n",
    "        C5_BP = self.C5.back_prop(reverse_flatten, momentum, weight_decay)\n",
    "        \n",
    "        S4_BP = self.S4.back_prop(C5_BP)\n",
    "        S4_BP = self.a2.back_prop(S4_BP)\n",
    "        C3_BP = self.C3.back_prop(S4_BP, momentum, weight_decay) \n",
    "        \n",
    "        S2_BP = self.S2.back_prop(C3_BP)\n",
    "        S2_BP = self.a1.back_prop(S2_BP)  \n",
    "        C1_BP = self.C1.back_prop(S2_BP, momentum, weight_decay)\n",
    "        \n",
    "    # Stochastic Diagonal Levenberg-Marquaedt method for determining the learning rate before the beginning of each epoch\n",
    "    def SDLM(self, mu, lr_global):\n",
    "        d2y_pred = self.Output.SDLM()\n",
    "        d2y_pred = self.a4.SDLM(d2y_pred)\n",
    "        \n",
    "        F6_SDLM = self.F6.SDLM(d2y_pred, mu, lr_global)\n",
    "        reverse_flatten = F6_SDLM[:,np.newaxis,np.newaxis,:]\n",
    "        \n",
    "        reverse_flatten = self.a3.SDLM(reverse_flatten) \n",
    "        C5_SDLM = self.C5.SDLM(reverse_flatten, mu, lr_global)\n",
    "        \n",
    "        S4_SDLM = self.S4.SDLM(C5_SDLM)\n",
    "        S4_SDLM = self.a2.SDLM(S4_SDLM)\n",
    "        C3_SDLM = self.C3.SDLM(S4_SDLM, mu, lr_global)\n",
    "        \n",
    "        S2_SDLM = self.S2.SDLM(C3_SDLM)\n",
    "        S2_SDLM = self.a1.SDLM(S2_SDLM)  \n",
    "        C1_SDLM = self.C1.SDLM(S2_SDLM, mu, lr_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNet = LeNet5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Diagonal Levenberg-Marquaedt method is used in the original LeNet5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of epoches & learning rate in the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_orig, lr_global_orig = 0, np.array([5e-4]*2 + [2e-4]*3 + [1e-4]*3 + [5e-5]*4 + [1e-5]*8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of epoches & learning rate I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches, lr_global_list = epoch_orig, lr_global_orig*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "weight_decay = 0\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "cost_last, count = np.Inf, 0\n",
    "err_rate_list = []\n",
    "for epoch in range(0,epoches):\n",
    "    print(\"---------- epoch\", epoch+1, \"begin ----------\")\n",
    "    \n",
    "    # Stochastic Diagonal Levenberg-Marquaedt method for determining the learning rate \n",
    "    (batch_image, batch_label) = random_mini_batches(train_image_normalized_pad, train_label, mini_batch_size = 500, one_batch=True)\n",
    "    ConvNet.Forward_Propagation(batch_image, batch_label, 'train')\n",
    "    lr_global = lr_global_list[epoch]\n",
    "    ConvNet.SDLM(0.02, lr_global)\n",
    "    \n",
    "    # print info\n",
    "    print(\"global learning rate:\", lr_global)\n",
    "    print(\"learning rates in trainable layers:\", np.array([ConvNet.C1.lr, ConvNet.C3.lr, ConvNet.C5.lr, ConvNet.F6.lr]))\n",
    "    print(\"batch size:\", batch_size)\n",
    "    print(\"Momentum:\",momentum,\", weight decay:\",weight_decay)\n",
    "    \n",
    "    #loop over each batch\n",
    "    ste = time.time()\n",
    "    cost = 0\n",
    "    mini_batches = random_mini_batches(train_image_normalized_pad, train_label, batch_size)\n",
    "    for i in range(len(mini_batches)):\n",
    "        batch_image, batch_label = mini_batches[i]\n",
    "        \n",
    "        loss = ConvNet.Forward_Propagation(batch_image, batch_label, 'train')     \n",
    "        cost += loss\n",
    "        \n",
    "        ConvNet.Back_Propagation(momentum, weight_decay) \n",
    "\n",
    "        # print progress\n",
    "        if i%(int(len(mini_batches)/100))==0:\n",
    "            sys.stdout.write(\"\\033[F\")   #CURSOR_UP_ONE\n",
    "            sys.stdout.write(\"\\033[K\")   #ERASE_LINE\n",
    "            print (\"progress:\", int(100*(i+1)/len(mini_batches)), \"%, \", \"cost =\", cost, end='\\r')\n",
    "    sys.stdout.write(\"\\033[F\")   #CURSOR_UP_ONE\n",
    "    sys.stdout.write(\"\\033[K\")   #ERASE_LINE\n",
    "    \n",
    "    print (\"Done, cost of epoch\", epoch+1, \":\", cost,\"                                             \")\n",
    "    \n",
    "    error01_train, _ = ConvNet.Forward_Propagation(train_image_normalized_pad, train_label, 'test')  \n",
    "    error01_test, _  = ConvNet.Forward_Propagation(test_image_normalized_pad,  test_label,  'test')     \n",
    "    err_rate_list.append([error01_train/60000, error01_test/10000])\n",
    "    print(\"0/1 error of training set:\",  error01_train, \"/\", len(train_label))\n",
    "    print(\"0/1 error of testing set: \",  error01_test,  \"/\", len(test_label))\n",
    "    print(\"Time used: \",time.time() - ste, \"sec\")\n",
    "    print(\"---------- epoch\", epoch+1, \"end ------------\")\n",
    "    with open('model_data_'+str(epoch)+'.pkl', 'wb') as output:\n",
    "        pickle.dump(ConvNet, output, pickle.HIGHEST_PROTOCOL)\n",
    "   \n",
    "    \n",
    "err_rate_list = np.array(err_rate_list).T\n",
    "print(\"Total time used: \", time.time() - st, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[30]:<br>\n",
    "########## if n_epoch=0 comment the following lines######## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the error rate of training and testing data after each epoch<br>\n",
    "#x = np.arange(epoches)\n",
    "#plt.xlabel('epoches')\n",
    "#plt.ylabel('error rate')\n",
    "#plt.plot(x, err_rate_list[0])\n",
    "#plt.plot(x, err_rate_list[1])\n",
    "#plt.legend(['training data', 'testing data'], loc='upper right')\n",
    "#plt.show()\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_data_13.pkl', 'rb') as input_:\n",
    "    ConvNet = pickle.load(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ############ for evaluation metrics in training ##################\n",
    "train_image_normalized_pad = normalize(zero_pad(train_image[:,:,:,np.newaxis], 2),'lenet5')\n",
    "error01, class_pred = ConvNet.Forward_Propagation(train_image_normalized_pad, train_label, 'test')  \n",
    "micro_f1 = micro_F1_SCORE(train_label,class_pred)\n",
    "print(\"micro F1 score for training = micro precision = micro recall = \"+str(micro_f1)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_form_y=hot_form(train_label,10)\n",
    "hot_form_pred=hot_form(class_pred,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_arr, precision_arr, recall_arr =f1_score_labels(hot_form_y ,hot_form_pred)\n",
    "print(\"f1 score for train = \"+str(f1_score_arr)+'\\n')\n",
    "print(\"precision for train = \"+str(precision_arr)+'\\n')\n",
    "print(\"recall for train = \"+str(recall_arr)+'\\n')\n",
    "macro_f1_score_train,macro_precision_train,macro_recall_train = macro_f1_score(f1_score_arr, precision_arr, recall_arr ,10)\n",
    "print(\"macro_f1_score for train =  \"+str(macro_f1_score_train)+'\\n')\n",
    "print(\"macro_precision for train =  \"+str(macro_precision_train)+'\\n')\n",
    "print(\"macro_recall for train =  \"+str(macro_recall_train)+'\\n')\n",
    "confusion_matrix_train=confusion_matrix(hot_form_y,hot_form_pred)\n",
    "print(\"confusion matrix for train --->\"+'\\n'+str(confusion_matrix_train)+'\\n')\n",
    "visualise_confusion_for_mnist(confusion_matrix_train)\n",
    "plt.show()\n",
    "    \n",
    " ###########################################################################   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## for evaluation metrics in testing ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_normalized_pad = normalize(zero_pad(test_image[:,:,:,np.newaxis], 2), 'lenet5')\n",
    "error01, class_pred = ConvNet.Forward_Propagation(test_image_normalized_pad, test_label, 'test')\n",
    "#print(class_pred)\n",
    "print(\"error rate:\", error01/len(class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1_test = micro_F1_SCORE(test_label,class_pred)\n",
    "print(\"micro F1 score for test = micro precision = micro recall = \"+str(micro_f1_test)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_form_y_test=hot_form(test_label,10)\n",
    "hot_form_pred_test=hot_form(class_pred,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_arr_test, precision_arr_test, recall_arr_test =f1_score_labels(hot_form_y_test ,hot_form_pred_test)\n",
    "print(\"f1 score for test = \"+str(f1_score_arr_test)+'\\n')\n",
    "print(\"precision for test = \"+str(precision_arr_test)+'\\n')\n",
    "print(\"recall for test = \"+str(recall_arr_test)+'\\n')\n",
    "macro_f1_score_test,macro_precision_test,macro_recall_test = macro_f1_score(f1_score_arr_test, precision_arr_test, recall_arr_test,10)\n",
    "print(\"macro_f1_score for test=  \"+str(macro_f1_score_test)+'\\n')\n",
    "print(\"macro_precision for test =  \"+str(macro_precision_test)+'\\n')\n",
    "print(\"macro_recall for test =  \"+str(macro_recall_test)+'\\n')\n",
    "confusion_matrix_test=confusion_matrix(hot_form_y_test,hot_form_pred_test)\n",
    "print(\"confusion matrix for test --->\"+'\\n'+str(confusion_matrix_test)+'\\n')\n",
    "visualise_confusion_for_mnist(confusion_matrix_test)\n",
    "plt.show()\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    ######### random selection from the training dataset ##############\n",
    "index = np.random.randint(60000,size=2)\n",
    "train_image_sample = train_image_normalized_pad[index,:,:,:]\n",
    "train_label_sample = train_label[index]\n",
    "print(\"Correct label:\",train_label_sample[0])\n",
    "plt.imshow(train_image_sample[0,:,:,0], cmap=mpl.cm.Greys)\n",
    "_, pred = ConvNet.Forward_Propagation(train_image_sample, train_label_sample, 'test')\n",
    "print(\"Predict label:\",pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(60000,size=2)\n",
    "train_image_sample = train_image_normalized_pad[index,:,:,:]\n",
    "train_label_sample = train_label[index]\n",
    "print(\"Correct label:\",train_label_sample[0])\n",
    "plt.imshow(train_image_sample[0,:,:,0], cmap=mpl.cm.Greys)\n",
    "_, pred = ConvNet.Forward_Propagation(train_image_sample, train_label_sample, 'test')\n",
    "print(\"Predict label:\",pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(60000,size=2)\n",
    "train_image_sample = train_image_normalized_pad[index,:,:,:]\n",
    "train_label_sample = train_label[index]\n",
    "print(\"Correct label:\",train_label_sample[0])\n",
    "plt.imshow(train_image_sample[0,:,:,0], cmap=mpl.cm.Greys)\n",
    "_, pred = ConvNet.Forward_Propagation(train_image_sample, train_label_sample, 'test')\n",
    "print(\"Predict label:\",pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(60000,size=2)\n",
    "train_image_sample = train_image_normalized_pad[index,:,:,:]\n",
    "train_label_sample = train_label[index]\n",
    "print(\"Correct label:\",train_label_sample[0])\n",
    "plt.imshow(train_image_sample[0,:,:,0], cmap=mpl.cm.Greys)\n",
    "_, pred = ConvNet.Forward_Propagation(train_image_sample, train_label_sample, 'test')\n",
    "print(\"Predict label:\",pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
