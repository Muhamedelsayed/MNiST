{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from layers import *\n", "from losses import CrossEntropyLoss\n", "from activations import ReLU\n", "from net import Net\n", "from Utils import *\n", "import math \n", "import matplotlib.pyplot as plt\n", "import matplotlib.image as mpimg\n", "import matplotlib as mpl\n", "from evaluation_matrix import *\n", "# GRADED FUNCTION: random_mini_batches\n", "  \n", "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n", "    \"\"\"\n", "    Creates a list of random minibatches from (X, Y)\n", "    \n", "    Arguments:\n", "    X -- input data, of shape (input size, number of examples)\n", "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n", "    mini_batch_size -- size of the mini-batches, integer\n", "    \n", "    Returns:\n", "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n", "    \"\"\"\n", "    \n", "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n", "    m = X.shape[1]                  # number of training examples\n", "    mini_batches = []\n", "        \n", "    # Step 1: Shuffle (X, Y)\n", "    permutation = list(np.random.permutation(m))\n", "    shuffled_X = X[:, permutation]\n", "    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n", "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n", "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n", "    for k in range(0, num_complete_minibatches):\n", "        ### START CODE HERE ### (approx. 2 lines)\n", "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n", "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n", "        ### END CODE HERE ###\n", "        mini_batch = (mini_batch_X.T, mini_batch_Y.T)\n", "        mini_batches.append(mini_batch)\n", "    \n", "    # Handling the end case (last mini-batch < mini_batch_size)\n", "    if m % mini_batch_size != 0:\n", "        ### START CODE HERE ### (approx. 2 lines)\n", "        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n", "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n", "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n", "        ### END CODE HERE ###\n", "        mini_batch = (mini_batch_X.T, mini_batch_Y.T)\n", "        mini_batches.append(mini_batch)\n", "    \n", "    return mini_batches"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train,y_train,_=load_data(\"mnist_train.csv\")\n", "X_test,y_test,_ = load_data(\"mnist_test.csv\")\n", "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "X_train, X_test = X_train.reshape(-1,X_train.shape[1]), X_test.reshape(-1,X_test.shape[1])\n", "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n", "X_dev,y_dev=X_train[55000:60000,:],y_train[55000:60000,:]\n", "X_train,y_train=X_train[0:55000,:],y_train[0:55000,:]\n", "net = Net(layers=[Linear(X_train.shape[1], 512,mode=1,layerNo=1),ReLU(), Linear(512, 512,mode=1,layerNo=2),ReLU(), Linear(512, 10,mode=1,layerNo=3)],\n", "          loss=CrossEntropyLoss())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reshaping<br>\n", "_train, X_test = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)<br>\n", "_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)<br>\n", "normalizing and scaling data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test,X_dev = X_train.astype('float32')/255, X_test.astype('float32')/255,X_dev.astype('float32')/255\n", "y_train, y_test,y_dev=y_train.astype('int8'), y_test.astype('int8'),y_dev.astype('int8')\n", "n_epochs = 0\n", "mini_batches = random_mini_batches(X_train.T, y_train.T)  \n", "i=0  \n", "accuracy_list = []\n", "for epoch_idx in range(n_epochs):\n", "    accuracy = 0\n", "    dev_accuracy = 0\n", "    for minibatch in mini_batches:\n", "        (minibatch_X, minibatch_Y) = minibatch\n", "        out = net(minibatch_X)\n", "        loss = net.loss(out, minibatch_Y)\n", "        net.backward()\n", "        net.update_weights(lr=0.1,i=i ,layer_type = 'linear')\n", "    if epoch_idx==n_epochs-1 :\n", "        i=1\n", "        net.update_weights(lr=0.1,i=i,layer_type = 'linear')\n", "    out = net(X_train)\n", "    preds = np.argmax(out, axis=1).reshape(-1, 1)\n", "    accuracy = 100*(preds == y_train).sum() / 55000\n", "    out = net(X_dev)\n", "    preds_dev = np.argmax(out, axis=1).reshape(-1, 1)\n", "    dev_accuracy = 100*(preds_dev == y_dev).sum() / 5000    \n", "    print(\"Epoch no. %d loss =  %2f4 \\t train_accuracy = %d %%\" % (epoch_idx + 1, loss, accuracy))\n", "    print('dev_accuracy = %d %%' % (dev_accuracy))\n", "    accuracy_list.append([accuracy, dev_accuracy])\n", "accuracy_list = np.array(accuracy_list).T"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["out = net(X_train)\n", "preds_train = np.argmax(out, axis=1).reshape(-1, 1)\n", "micro_f1 = micro_F1_SCORE(y_train,preds_train)\n", "print(\"micro F1 score for training = micro precision = micro recall = \"+str(micro_f1)+'\\n')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hot_form_y=hot_form(y_train,10)\n", "hot_form_pred=hot_form(preds_train,10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f1_score_arr, precision_arr, recall_arr =f1_score_labels(hot_form_y ,hot_form_pred)\n", "print(\"f1 score for train = \"+str(f1_score_arr)+'\\n')\n", "print(\"precision for train = \"+str(precision_arr)+'\\n')\n", "print(\"recall for train = \"+str(recall_arr)+'\\n')\n", "macro_f1_score_train,macro_precision_train,macro_recall_train = macro_f1_score(f1_score_arr, precision_arr, recall_arr ,10)\n", "print(\"macro_f1_score for train =  \"+str(macro_f1_score_train)+'\\n')\n", "print(\"macro_precision for train =  \"+str(macro_precision_train)+'\\n')\n", "print(\"macro_recall for train =  \"+str(macro_recall_train)+'\\n')\n", "confusion_matrix_train=confusion_matrix(hot_form_y,hot_form_pred)\n", "print(\"confusion matrix for train --->\"+'\\n'+str(confusion_matrix_train)+'\\n')\n", "visualise_confusion_for_mnist(confusion_matrix_train)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["########## if mode =1 and n_epoch=0 comment the following lines######## <br>\n", "x = np.arange(n_epochs)<br>\n", "plt.xlabel('epoches')<br>\n", "plt.ylabel('accuracy')<br>\n", "plt.plot(x, accuracy_list[0])<br>\n", "plt.plot(x, accuracy_list[1])<br>\n", "plt.legend(['training data', 'dev data'], loc='upper left')<br>\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###################################################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_accuracy = 0\n", "out = net(X_test)\n", "preds_test = np.argmax(out, axis=1).reshape(-1, 1)\n", "test_accuracy = 100*(preds_test == y_test).sum() / 10000\n", "print('test_accuracy = %d %%' % (test_accuracy))\n", "# print((y_test==preds_test).all())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["preds_test = np.argmax(out, axis=1).reshape(-1, 1)\n", "micro_f1_test = micro_F1_SCORE(y_test,preds_test)\n", "print(\"micro F1 score for test = micro precision = micro recall = \"+str(micro_f1_test)+'\\n')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hot_form_y_test=hot_form(y_test,10)\n", "hot_form_pred_test=hot_form(preds_test,10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f1_score_arr_test, precision_arr_test, recall_arr_test =f1_score_labels(hot_form_y_test ,hot_form_pred_test)\n", "print(\"f1 score for test = \"+str(f1_score_arr_test)+'\\n')\n", "print(\"precision for test = \"+str(precision_arr_test)+'\\n')\n", "print(\"recall for test = \"+str(recall_arr_test)+'\\n')\n", "macro_f1_score_test,macro_precision_test,macro_recall_test = macro_f1_score(f1_score_arr_test, precision_arr_test, recall_arr_test,10)\n", "print(\"macro_f1_score for test=  \"+str(macro_f1_score_test)+'\\n')\n", "print(\"macro_precision for test =  \"+str(macro_precision_test)+'\\n')\n", "print(\"macro_recall for test =  \"+str(macro_recall_test)+'\\n')\n", "confusion_matrix_test=confusion_matrix(hot_form_y_test,hot_form_pred_test)\n", "print(\"confusion matrix for test --->\"+'\\n'+str(confusion_matrix_test)+'\\n')\n", "visualise_confusion_for_mnist(confusion_matrix_test)\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}