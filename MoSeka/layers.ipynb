{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from math import sqrt\n", "from itertools import product"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from Utils import zero_pad"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Function:\n", "    \"\"\"\n", "    Abstract model of a differentiable function.\n", "    \"\"\"\n", "    def __init__(self, *args, **kwargs):\n", "        # initializing cache for intermediate results\n", "        # helps with gradient calculation in some cases\n", "        self.cache = {}\n", "        # cache for gradients\n", "        self.grad = {}\n", "    def __call__(self, *args, **kwargs):\n", "        # calculating output\n", "        output = self.forward(*args, **kwargs)\n", "        # calculating and caching local gradients\n", "        self.grad = self.local_grad(*args, **kwargs)\n", "        return output\n", "    def forward(self, *args, **kwargs):\n", "        \"\"\"\n", "        Forward pass of the function. Calculates the output value and the\n", "        gradient at the input as well.\n", "        \"\"\"\n", "        pass\n", "    def backward(self, *args, **kwargs):\n", "        \"\"\"\n", "        Backward pass. Computes the local gradient at the input value\n", "        after forward pass.\n", "        \"\"\"\n", "        pass\n", "    def local_grad(self, *args, **kwargs):\n", "        \"\"\"\n", "        Calculates the local gradients of the function at the given input.\n", "        Returns:\n", "            grad: dictionary of local gradients.\n", "        \"\"\"\n", "        pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Layer(Function): #used in case that the layer have weights \n", "    \"\"\"\n", "    Abstract model of a neural network layer. In addition to Function, a Layer\n", "    also has weights and gradients with respect to the weights.\n", "    \"\"\"\n", "    def __init__(self, *args, **kwargs):\n", "        super().__init__(*args, **kwargs)\n", "        self.weight = {}\n", "        self.weight_update = {}\n", "    def _init_weights(self, *args, **kwargs):\n", "        pass\n", "    def _update_weights(self, lr,i,j,layer_type):\n", "        \"\"\"\n", "        Updates the weights using the corresponding _global_ gradients computed during\n", "        backpropagation.\n", "        Args:\n", "             lr: float. Learning rate.\n", "        \"\"\"\n", "        axis = 0\n", "        if i==0:\n", "            for weight_key, weight in self.weight.items():\n", "                self.weight[weight_key] = self.weight[weight_key] - lr * self.weight_update[weight_key]\n", "        elif i==1:\n", "            if layer_type == 'linear':\n", "                axis = 0\n", "            if layer_type == 'conv':\n", "                axis = 1\n", "            X=self.weight\n", "            vol_shape = X['W'].shape[1:]\n", "            n_voxels = np.prod(vol_shape)\n", "            W_reshaped=np.reshape(X['W'],(X['W'].shape[0], int(n_voxels)))\n", "            weiiights=np.concatenate((W_reshaped,X['b']),axis=axis)\n", "            #weiiights=np.concatenate((X['W'],X['b']),axis=0)\n", "            np.savetxt(layer_type+\" Layer\"+ str(j) + \".csv\", weiiights, delimiter=\",\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Flatten(Function):\n", "    def forward(self, X):\n", "        self.cache['shape'] = X.shape\n", "        n_batch = X.shape[0] \n", "        return X.reshape(n_batch, -1)\n", "    def backward(self, dY):\n", "        return dY.reshape(self.cache['shape'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MaxPool2D(Function):\n", "    def __init__(self, kernel_size=(2, 2)):\n", "        super().__init__()\n", "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n", "    def __call__(self, X):\n", "        # in contrary to other Function subclasses, MaxPool2D does not need to call\n", "        # .local_grad() after forward pass because the gradient is calculated during it\n", "        return self.forward(X)\n", "    def forward(self, X):\n", "        N, C, H, W = X.shape\n", "        KH, KW = self.kernel_size\n", "        grad = np.zeros_like(X)\n", "        Y = np.zeros((N, C, H//KH, W//KW))\n\n", "        # for n in range(N):\n", "        for h, w in product(range(0, H//KH), range(0, W//KW)):\n", "            h_offset, w_offset = h*KH, w*KW\n", "            rec_field = X[:, :, h_offset:h_offset+KH, w_offset:w_offset+KW]\n", "            Y[:, :, h, w] = np.max(rec_field, axis=(2, 3))\n", "            for kh, kw in product(range(KH), range(KW)):\n", "                grad[:, :, h_offset+kh, w_offset+kw] = (X[:, :, h_offset+kh, w_offset+kw] >= Y[:, :, h, w])\n\n", "        # storing the gradient\n", "        self.grad['X'] = grad\n", "        return Y\n", "    def backward(self, dY):\n", "        dY = np.repeat(np.repeat(dY, repeats=self.kernel_size[0], axis=2),\n", "                       repeats=self.kernel_size[1], axis=3)\n", "        return self.grad['X']*dY\n", "    def local_grad(self, X):\n", "        # small hack: because for MaxPool calculating the gradient is simpler during\n", "        # the forward pass, it is calculated there and this function just returns the\n", "        # grad dictionary\n", "        return self.grad"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BatchNorm2D(Layer):\n", "    def __init__(self, n_channels, epsilon=1e-5):\n", "        super().__init__()\n", "        self.epsilon = epsilon\n", "        self.n_channels = n_channels\n", "        self._init_weights(n_channels)\n", "    def _init_weights(self, n_channels):\n", "        self.weight['gamma'] = np.ones(shape=(1, n_channels, 1, 1))\n", "        self.weight['beta'] = np.zeros(shape=(1, n_channels, 1, 1))\n", "    def forward(self, X):\n", "        \"\"\"\n", "        Forward pass for the 2D batchnorm layer.\n", "        Args:\n", "            X: numpy.ndarray of shape (n_batch, n_channels, height, width).\n", "        Returns_\n", "            Y: numpy.ndarray of shape (n_batch, n_channels, height, width).\n", "                Batch-normalized tensor of X.\n", "        \"\"\"\n", "        mean = np.mean(X, axis=(2, 3), keepdims=True)\n", "        var = np.var(X, axis=(2, 3), keepdims=True) + self.epsilon\n", "        invvar = 1.0/var\n", "        sqrt_invvar = np.sqrt(invvar)\n", "        centered = X - mean\n", "        scaled = centered * sqrt_invvar\n", "        normalized = scaled * self.weight['gamma'] + self.weight['beta']\n\n", "        # caching intermediate results for backprop\n", "        self.cache['mean'] = mean\n", "        self.cache['var'] = var\n", "        self.cache['invvar'] = invvar\n", "        self.cache['sqrt_invvar'] = sqrt_invvar\n", "        self.cache['centered'] = centered\n", "        self.cache['scaled'] = scaled\n", "        self.cache['normalized'] = normalized\n", "        return normalized\n", "    def backward(self, dY):\n", "        \"\"\"\n", "        Backward pass for the 2D batchnorm layer. Calculates global gradients\n", "        for the input and the parameters.\n", "        Args:\n", "            dY: numpy.ndarray of shape (n_batch, n_channels, height, width).\n", "        Returns:\n", "            dX: numpy.ndarray of shape (n_batch, n_channels, height, width).\n", "                Global gradient wrt the input X.\n", "        \"\"\"\n", "        # global gradients of parameters\n", "        dgamma = np.sum(self.cache['scaled'] * dY, axis=(0, 2, 3), keepdims=True)\n", "        dbeta = np.sum(dY, axis=(0, 2, 3), keepdims=True)\n\n", "        # caching global gradients of parameters\n", "        self.weight_update['gamma'] = dgamma\n", "        self.weight_update['beta'] = dbeta\n\n", "        # global gradient of the input\n", "        dX = self.grad['X'] * dY\n", "        return dX\n", "    def local_grad(self, X):\n", "        \"\"\"\n", "        Calculates the local gradient for X.\n", "        Args:\n", "            dY: numpy.ndarray of shape (n_batch, n_channels, height, width).\n", "        Returns:\n", "            grads: dictionary of gradients.\n", "        \"\"\"\n", "        # global gradient of the input\n", "        N, C, H, W = X.shape\n", "        # ppc = pixels per channel, useful variable for further computations\n", "        ppc = H * W\n\n", "        # gradient for 'denominator path'\n", "        dsqrt_invvar = self.cache['centered']\n", "        dinvvar = (1.0 / (2.0 * np.sqrt(self.cache['invvar']))) * dsqrt_invvar\n", "        dvar = (-1.0 / self.cache['var'] ** 2) * dinvvar\n", "        ddenominator = (X - self.cache['mean']) * (2 * (ppc - 1) / ppc ** 2) * dvar\n\n", "        # gradient for 'numerator path'\n", "        dcentered = self.cache['sqrt_invvar']\n", "        dnumerator = (1.0 - 1.0 / ppc) * dcentered\n", "        dX = ddenominator + dnumerator\n", "        grads = {'X': dX}\n", "        return grads"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Linear(Layer):\n", "    def __init__(self, in_dim, out_dim,mode,layerNo):\n", "        super().__init__()\n", "        self._init_weights(in_dim, out_dim,mode,layerNo)\n", "    def _init_weights(self, in_dim, out_dim,mode,layerNo):\n", "        scale = 1 / sqrt(in_dim)\n", "        if mode==0:\n", "            self.weight['W'] = scale * np.random.randn(in_dim, out_dim)\n", "            self.weight['b'] = scale * np.random.randn(1, out_dim)\n", "        if (mode==1):\n", "            data = pd.read_csv(\"Linear Layer\"+str(layerNo)+\".csv\",low_memory=False ,header=None)\n", "            data = np.asarray(data)\n", "            self.weight['W'] = data[0:data.shape[0]-1,:]\n", "            self.weight['b'] = data[data.shape[0]-1:data.shape[0],:]\n", "    def forward(self, X):\n", "        \"\"\"\n", "        Forward pass for the Linear layer.\n", "        Args:\n", "            X: numpy.ndarray of shape (n_batch, in_dim) containing\n", "                the input value.\n", "        Returns:\n", "            Y: numpy.ndarray of shape of shape (n_batch, out_dim) containing\n", "                the output value.\n", "        \"\"\"\n", "        output = np.dot(X, self.weight['W']) + self.weight['b']\n\n", "        # caching variables for backprop\n", "        self.cache['X'] = X\n", "        self.cache['output'] = output\n", "        return output\n", "    def backward(self, dY):\n", "        \"\"\"\n", "        Backward pass for the Linear layer.\n", "        Args:\n", "            dY: numpy.ndarray of shape (n_batch, n_out). Global gradient\n", "                backpropagated from the next layer.\n", "        Returns:\n", "            dX: numpy.ndarray of shape (n_batch, n_out). Global gradient\n", "                of the Linear layer.\n", "        \"\"\"\n", "        # calculating the global gradient, to be propagated backwards\n", "        dX = dY.dot(self.grad['X'].T)\n", "        # calculating the global gradient wrt to weights\n", "        X = self.cache['X']\n", "        dW = self.grad['W'].T.dot(dY)\n", "        db = np.sum(dY, axis=0, keepdims=True)\n", "        # caching the global gradients\n", "        self.weight_update = {'W': dW, 'b': db}\n", "        return dX\n", "    def local_grad(self, X):\n", "        \"\"\"\n", "        Local gradients of the Linear layer at X.\n", "        Args:\n", "            X: numpy.ndarray of shape (n_batch, in_dim) containing the\n", "                input data.\n", "        Returns:\n", "            grads: dictionary of local gradients with the following items:\n", "                X: numpy.ndarray of shape (n_batch, in_dim).\n", "                W: numpy.ndarray of shape (n_batch, in_dim).\n", "                b: numpy.ndarray of shape (n_batch, 1).\n", "        \"\"\"\n", "        gradX_local = self.weight['W']\n", "        gradW_local = X\n", "        gradb_local = np.ones_like(self.weight['b'])\n", "        grads = {'X': gradX_local, 'W': gradW_local, 'b': gradb_local}\n", "        return grads"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}
