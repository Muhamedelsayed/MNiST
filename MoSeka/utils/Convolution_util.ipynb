{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np \n", "from scipy.signal import convolve2d\n", "from utils.utils_func import zero_pad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Numpy version: compute with np.tensordot()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_forward(A_prev, W, b, hparameters):\n", "    \"\"\"\n", "    Implements the forward propagation for a convolution function\n", "    \n", "    Arguments:\n", "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev) \n", "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n", "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n", "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n", "        \n", "    Returns:\n", "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n", "    cache -- cache of values needed for the conv_backward() function\n", "    \"\"\"\n", "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n", "    (f, f, n_C_prev, n_C) = W.shape\n", "    \n", "    stride = hparameters[\"stride\"]\n", "    pad = hparameters[\"pad\"]\n", "    \n", "    n_H = int((n_H_prev + 2*pad - f)/stride + 1)\n", "    n_W = int((n_W_prev + 2*pad - f)/stride + 1)\n", "    \n", "    # Initialize the output volume Z with zeros. \n", "    Z = np.zeros((m, n_H, n_W, n_C))\n", "    A_prev_pad = zero_pad(A_prev, pad)\n", "    for h in range(n_H):                            # loop over vertical axis of the output volume\n", "        for w in range(n_W):                        # loop over horizontal axis of the output volume\n", "            # Use the corners to define the (3D) slice of a_prev_pad.\n", "            A_slice_prev = A_prev_pad[:, h*stride:h*stride+f, w*stride:w*stride+f, :]\n", "            #print(np.tensordot(A_slice_prev, W, axes=([1,2,3],[0,1,2])).shape, b.shape)\n", "            # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron.\n", "            Z[:, h, w, :] = np.tensordot(A_slice_prev, W, axes=([1,2,3],[0,1,2])) + b\n", "                            \n", "    assert(Z.shape == (m, n_H, n_W, n_C))\n", "    cache = (A_prev, W, b, hparameters)\n", "    return Z, cache"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Numpy version: compute with np.dot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_backward(dZ, cache):\n", "    \"\"\"\n", "    Implement the backward propagation for a convolution function\n", "    \n", "    Arguments:\n", "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n", "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n", "    \n", "    Returns:\n", "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n", "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n", "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n", "          numpy array of shape (f, f, n_C_prev, n_C)\n", "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n", "          numpy array of shape (1, 1, 1, n_C)\n", "    \"\"\"\n", "    (A_prev, W, b, hparameters) = cache\n", "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n", "    (f, f, n_C_prev, n_C) = W.shape\n", "    stride = hparameters[\"stride\"]\n", "    pad = hparameters[\"pad\"]\n", "    \n", "    (m, n_H, n_W, n_C) = dZ.shape\n", "    \n", "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n", "    dW = np.zeros((f, f, n_C_prev, n_C))\n", "    db = np.zeros((1, 1, 1, n_C))\n", "    \n", "    if pad != 0:\n", "        A_prev_pad = zero_pad(A_prev, pad)\n", "        dA_prev_pad = zero_pad(dA_prev, pad)\n", "    else:\n", "        A_prev_pad = A_prev\n", "        dA_prev_pad = dA_prev\n", "    \n", "    for h in range(n_H):                    # loop over vertical axis of the output volume\n", "        for w in range(n_W):                # loop over horizontal axis of the output volume\n", "            # Find the corners of the current \"slice\"\n", "            vert_start, horiz_start  = h*stride, w*stride\n", "            vert_end,   horiz_end    = vert_start+f, horiz_start+f\n", "            \n", "            # Use the corners to define the slice from a_prev_pad\n", "            A_slice = A_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :]\n", "            \n", "            # Update gradients for the window and the filter's parameters\n", "            dA_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :] += np.transpose(np.dot(W, dZ[:, h, w, :].T), (3,0,1,2))\n", "            dW += np.dot(np.transpose(A_slice, (1,2,3,0)), dZ[:, h, w, :])\n", "            db += np.sum(dZ[:, h, w, :], axis=0)\n", "            \n", "    # Set dA_prev to the unpaded dA_prev_pad\n", "    dA_prev = dA_prev_pad if pad == 0 else dA_prev_pad[:,pad:-pad, pad:-pad, :]\n", "        \n", "    # Making sure your output shape is correct\n", "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n", "    \n", "    return dA_prev, dW, db"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_SDLM(dZ, cache):\n", "    (A_prev, W, b, hparameters) = cache\n", "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n", "    (f, f, n_C_prev, n_C) = W.shape\n", "    stride = hparameters[\"stride\"]\n", "    pad = hparameters[\"pad\"]\n", "    (m, n_H, n_W, n_C) = dZ.shape\n", "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n", "    dW = np.zeros((f, f, n_C_prev, n_C))\n", "    db = np.zeros((1, 1, 1, n_C))\n", "    \n", "    if pad != 0:\n", "        A_prev_pad = zero_pad(A_prev, pad)\n", "        dA_prev_pad = zero_pad(dA_prev, pad)\n", "    else:\n", "        A_prev_pad = A_prev\n", "        dA_prev_pad = dA_prev\n", "    \n", "    for h in range(n_H):                    # loop over vertical axis of the output volume\n", "        for w in range(n_W):                # loop over horizontal axis of the output volume\n", "            # Find the corners of the current \"slice\"\n", "            vert_start, horiz_start  = h*stride, w*stride\n", "            vert_end,   horiz_end    = vert_start+f, horiz_start+f\n", "            \n", "            # Use the corners to define the slice from a_prev_pad\n", "            A_slice = A_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :]\n", "            \n", "            # Update gradients for the window and the filter's parameters\n", "            dA_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :] += np.transpose(np.dot(np.power(W,2), dZ[:, h, w, :].T), (3,0,1,2))\n", "            dW += np.dot(np.transpose(np.power(A_slice,2), (1,2,3,0)), dZ[:, h, w, :])\n", "    # Set dA_prev to the unpaded dA_prev_pad\n", "    dA_prev = dA_prev_pad if pad == 0 else dA_prev_pad[:,pad:-pad, pad:-pad, :]\n", "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n", "    return dA_prev, dW"]}, {"cell_type": "markdown", "metadata": {}, "source": ["################################# functions below are NOT USED in training ################################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_single_step(a_slice_prev, W, b):\n", "    \"\"\"\n", "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n", "    of the previous layer.\n", "    \n", "    Arguments:\n", "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n", "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n", "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n", "    \n", "    Returns:\n", "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n", "    \"\"\"\n", "    # Element-wise product between a_slice and W. Do not add the bias yet.\n", "    s = a_slice_prev * W\n", "    # Sum over all entries of the volume s.\n", "    Z = np.sum(s)\n", "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n", "    Z = Z + b\n", "    return Z"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Original version from Andrew Ng's course, high readability but extremely slow "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_forward_orig(A_prev, W, b, hparameters):\n", "    \"\"\"\n", "    Implements the forward propagation for a convolution function\n", "    \n", "    Arguments:\n", "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n", "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n", "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n", "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n", "        \n", "    Returns:\n", "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n", "    cache -- cache of values needed for the conv_backward() function\n", "    \"\"\"\n", "    \n", "    ### START CODE HERE ###\n", "    # Retrieve dimensions from A_prev's shape (\u00e2\u2030\u02c61 line)  \n", "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n", "    \n", "    # Retrieve dimensions from W's shape \n", "    (f, f, n_C_prev, n_C) = W.shape\n", "    \n", "    # Retrieve information from \"hparameters\"\n", "    stride = hparameters[\"stride\"]\n", "    pad = hparameters[\"pad\"]\n", "    \n", "    # Compute the dimensions of the CONV output volume using the formula given above. \n", "    n_H = int((n_H_prev + 2*pad - f)/stride + 1)\n", "    n_W = int((n_W_prev + 2*pad - f)/stride + 1)\n", "    \n", "    # Initialize the output volume Z with zeros. \n", "    Z = np.zeros((m, n_H, n_W, n_C))\n", "    \n", "    # Create A_prev_pad by padding A_prev\n", "    A_prev_pad = zero_pad(A_prev, pad)\n", "    \n", "    for i in range(m):                                  # loop over the batch of training examples\n", "        a_prev_pad = A_prev_pad[i,:,:,:]                # Select ith training example's padded activation\n", "        for h in range(n_H):                            # loop over vertical axis of the output volume\n", "            for w in range(n_W):                        # loop over horizontal axis of the output volume\n", "                for c in range(n_C):                    # loop over channels (= #filters) of the output volume\n", "                    \n", "                    # Find the corners of the current \"slice\"\n", "                    vert_start  = h*stride\n", "                    vert_end    = vert_start+f\n", "                    horiz_start = w*stride\n", "                    horiz_end   = horiz_start+f\n", "                    \n", "                    # Use the corners to define the (3D) slice of a_prev_pad\n", "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n", "                    \n", "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n", "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n", "                                        \n", "    # Making sure your output shape is correct\n", "    assert(Z.shape == (m, n_H, n_W, n_C))\n", "    \n", "    # Save information in \"cache\" for the backprop\n", "    cache = (A_prev, W, b, hparameters)\n", "    \n", "    return Z, cache"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Conv3D(image3D, filter3D, b, stride):\n", "    \"\"\"\n", "    arguments:\n", "    image3D     shape = (n_H_prev, n_W_prev, n_C_prev)\n", "    filter3D    shape = (f, f, n_C_prev, n_C)\n", "    b           shape = (1, 1, 1,        n_C)\n", "    \n", "    return :\n", "    output      shape = (_, _, n_C)        \n", "    \"\"\"\n", "    output = []\n", "    n_C_prev, n_C = filter3D.shape[2], filter3D.shape[3]\n", "    for c in range(n_C):\n", "        output_c = 0\n", "        for c_prev in range(n_C_prev):\n", "            output_c += convolve2d(image3D[:,:,c_prev], np.rot90(filter3D[:,:,c_prev,c],2),'valid')[::stride,::stride] \n", "        output_c += b[0,0,0,c]\n", "        output += [output_c]\n", "    return np.transpose(np.array(output),(1,2,0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scipy version convolution: slightly faster than original version <br>\n", "but still slower than the high-dimensional matrix computation version"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_forward_scipy(A_prev, W, b, hparameters):\n", "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n", "    (f, f, n_C_prev, n_C) = W.shape\n", "    \n", "    stride = hparameters[\"stride\"]\n", "    pad = hparameters[\"pad\"]\n", "    \n", "    n_H = int((n_H_prev + 2*pad - f)/stride + 1)\n", "    n_W = int((n_W_prev + 2*pad - f)/stride + 1)\n", "    \n", "    A_prev_pad = zero_pad(A_prev, pad)\n", "    \n", "    Z = np.empty((m, n_H, n_W, n_C))\n", "    for i in range(m): \n", "        Z[i,:,:,:] = Conv3D(A_prev_pad[i,:,:,:], W, b, stride)\n", "                                        \n", "    assert(Z.shape == (m, n_H, n_W, n_C))\n", "    cache = (A_prev, W, b, hparameters)\n", "    return Z, cache"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Original version from Andrew Ng's course, high readability but extremely slow "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conv_backward_orig(dZ, cache):\n", "    \"\"\"\n", "    Implement the backward propagation for a convolution function\n", "    \n", "    Arguments:\n", "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n", "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n", "    \n", "    Returns:\n", "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n", "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n", "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n", "          numpy array of shape (f, f, n_C_prev, n_C)\n", "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n", "          numpy array of shape (1, 1, 1, n_C)\n", "    \"\"\"\n", "    (A_prev, W, b, hparameters) = cache\n", "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n", "    (f, f, n_C_prev, n_C) = W.shape\n", "    stride = hparameters[\"stride\"]\n", "    pad = hparameters[\"pad\"]\n", "    \n", "    (m, n_H, n_W, n_C) = dZ.shape\n", "    \n", "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n", "    dW = np.zeros((f, f, n_C_prev, n_C))\n", "    db = np.zeros((1, 1, 1, n_C))\n", "    A_prev_pad = zero_pad(A_prev, pad)\n", "    dA_prev_pad = zero_pad(dA_prev, pad)\n", "    \n", "    for i in range(m):                          # loop over the training examples\n", "        # select ith training example from A_prev_pad and dA_prev_pad\n", "        a_prev_pad = A_prev_pad[i,:,:,:]\n", "        da_prev_pad = dA_prev_pad[i,:,:,:]\n", "        \n", "        for h in range(n_H):                    # loop over vertical axis of the output volume\n", "            for w in range(n_W):                # loop over horizontal axis of the output volume\n", "                for c in range(n_C):            # loop over the channels of the output volume\n", "                    \n", "                    # Find the corners of the current \"slice\"\n", "                    vert_start  = h*stride\n", "                    vert_end    = vert_start+f\n", "                    horiz_start = w*stride\n", "                    horiz_end   = horiz_start+f\n", "                    \n", "                    # Use the corners to define the slice from a_prev_pad\n", "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n", "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n", "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n", "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n", "                    db[:,:,:,c] += dZ[i, h, w, c]\n", "                    \n", "        # Set the ith training example's dA_prev to the unpaded da_prev_pad\n", "        if pad != 0:\n", "            dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n", "        else:\n", "            dA_prev[i, :, :, :] = da_prev_pad\n", "    \n", "    # Making sure your output shape is correct\n", "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n", "    \n", "    return dA_prev, dW, db"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}
