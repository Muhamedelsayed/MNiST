{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sn\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "#from Utils import  load_data\n", "#from dense import  net\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import svm, datasets\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import plot_confusion_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y=   [2,3,4,5,1,0,0,1,2,3,4,5]\n", "pred=[1,3,2,5,4,0,1,1,2,3,4,5]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def micro_F1_SCORE  (y , pred):\n", "    # micro_F1_SCORE == recall == precision\n", "    TP = 0\n", "    FP=0\n", "    for i in range (len(y)):\n", "        if y[i] == pred[i]:\n", "            TP+=1\n", "        else:\n", "            FP+=1\n", "    return TP/(TP+FP)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def hot_form (y, labels_num ):\n\n", "    #new_mat = [[0 for x in range(labels_num)] for y in range(len (y))]\n", "    new_mat = np.empty((len (y), labels_num))\n", "    for i in range (len (y)):\n", "        for j in range (labels_num):\n", "            if ( j == y[i] ):\n", "                new_mat[i][j]= 1\n", "            else :\n", "                new_mat[i][j]=0\n", "    return new_mat"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def f1_score_labels (y_hot_form ,pred_hot_form):\n", "        num_of_exam =y_hot_form.shape[0]\n", "        labels_num =y_hot_form.shape[1]\n", "        TP= [0]*labels_num     #[ [0] * labels_num for _ in range(num_of_exam)]\n", "        FP=[0]*labels_num\n", "        TN=[0]*labels_num\n", "        FN=[0]*labels_num\n", "        precision=[0]*labels_num\n", "        recall=[0]*labels_num\n", "        f1_score=[0]*labels_num\n", "        for i in range (num_of_exam):\n", "            for j in range (labels_num):\n", "                if ( y_hot_form[i][j] == 1 and pred_hot_form[i][j]== 1):\n", "                    TP[j]+= 1\n", "                elif( y_hot_form[i][j] == 0 and pred_hot_form [i][j] == 0) :\n", "                    TN[j]+= 1\n", "                elif( y_hot_form[i][j] == 1 and pred_hot_form [i][j] == 0) :\n", "                    FN[j]+= 1\n", "                elif( y_hot_form[i][j] == 0 and pred_hot_form [i][j] == 1) :\n", "                    FP[j]+= 1\n", "        for i in range (6):\n", "            precision[i] = TP[i]/(TP[i]+FP[i])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            recall[i]=TP[i]/(TP[i]+FN[i])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            f1_score[i] =2 * (precision[i] * recall[i])/(precision[i] + recall[i])\n", "        return f1_score, precision, recall"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def macro_f1_score(f1_score_arr,precision_arr,recall_arr,labels_num):\n", "    macro_f1_score=sum(f1_score_arr)/labels_num\n", "    macro_precision=sum(precision_arr)/labels_num\n", "    macro_recall=sum(recall_arr)/labels_num\n", "    return macro_f1_score,macro_precision,macro_recall"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def confusion_matrix(y_hot_form,pred_hot_form):\n", "    num_of_exam =y_hot_form.shape[0]\n", "    labels_num =y_hot_form.shape[1]\n", "    confusion_matrix =[ [0] * labels_num for _ in range(labels_num)]\n", "    for i in range (num_of_exam):\n", "        for j in range(labels_num):\n", "            if (y_hot_form[i][j]==1):\n", "                col=j\n", "                break\n", "        for j in range(labels_num):\n", "            if (pred_hot_form[i][j]==1):\n", "                row=j\n", "                break\n", "        confusion_matrix[row][col]+=1\n", "    return confusion_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualise_confusion(mat):\n", "    array = [[33,2,0,0,0,0,0,0,0,1,3],\n", "            [3,31,0,0,0,0,0,0,0,0,0],\n", "            [0,4,41,0,0,0,0,0,0,0,1],\n", "            [0,1,0,30,0,6,0,0,0,0,1],\n", "            [0,0,0,0,38,10,0,0,0,0,0],\n", "            [0,0,0,3,1,39,0,0,0,0,4],\n", "            [0,2,2,0,4,1,31,0,0,0,2],\n", "            [0,1,0,0,0,0,0,36,0,2,0],\n", "            [0,0,0,0,0,0,1,5,37,5,1],\n", "            [3,0,0,0,0,0,0,0,0,39,0],\n", "            [0,0,0,0,0,0,0,0,0,0,38]]\n", "    df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"],\n", "                  columns = [i for i in \"ABCDEFGHIJK\"])\n", "    plt.figure(figsize = (10,7))\n", "    #sn.heatmap(df_cm, annot=True)\n", "    plt.matshow(df_cm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "X_train,y_train,_=load_data(\"mnist_train.csv\")<br>\n", "X_test,y_test,_ = load_data(\"mnist_test.csv\")<br>\n", "# (X_train, y_train), (X_test, y_test) = mnist.load_data()<br>\n", "X_train, X_test = X_train.reshape(-1,X_train.shape[1]), X_test.reshape(-1,X_test.shape[1])<br>\n", "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)<br>\n", "# normalizing and scaling data<br>\n", "X_train, X_test = X_train.astype('float32')/255, X_test.astype('float32')/255<br>\n", "y_train, y_test=y_train.astype('int8'), y_test.astype('int8')<br>\n", "out = net(X_test)<br>\n", "preds_test = np.argmax(out, axis=1).reshape(-1, 1)<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["micro_f1 = micro_F1_SCORE(y,pred)\n", "print(micro_f1)\n", "print(\"##############################################################################\")\n", "print(\"##############################################################################\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hot_form_y=hot_form(y,6)\n", "hot_form_pred=hot_form(pred,6)\n", "print(hot_form_y)\n", "print(hot_form_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"##############################################################################\")\n", "print(\"##############################################################################\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f1_score_arr, precision_arr, recall_arr =f1_score_labels(hot_form_y ,hot_form_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f1_score_arr)\n", "print(precision_arr)\n", "print(recall_arr)\n", "print(\"##############################################################################\")\n", "print(\"##############################################################################\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["macro_f1_score,macro_precision,macro_recall = macro_f1_score(f1_score_arr, precision_arr, recall_arr ,5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(macro_f1_score)\n", "print(macro_precision)\n", "print(macro_recall)\n", "print(\"##############################################################################\")\n", "print(\"##############################################################################\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["confusion_matrix=confusion_matrix(hot_form_y,hot_form_pred)\n", "print(confusion_matrix)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"##############################################################################\")\n", "print(\"##############################################################################\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = [[33,2,0,0,0,0,0,0,0,1,3],\n", "            [3,31,0,0,0,0,0,0,0,0,0],\n", "            [0,4,41,0,0,0,0,0,0,0,1],\n", "            [0,1,0,30,0,6,0,0,0,0,1],\n", "            [0,0,0,0,38,10,0,0,0,0,0],\n", "            [0,0,0,3,1,39,0,0,0,0,4],\n", "            [0,2,2,0,4,1,31,0,0,0,2],\n", "            [0,1,0,0,0,0,0,36,0,2,0],\n", "            [0,0,0,0,0,0,1,5,37,5,1],\n", "            [3,0,0,0,0,0,0,0,0,39,0],\n", "            [0,0,0,0,0,0,0,0,0,0,38]]\n", "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"],\n", "                  columns = [i for i in \"ABCDEFGHIJK\"])\n", "plt.figure(figsize = (10,7))\n", "sn.heatmap(df_cm, annot=True)\n", "#plt.matshow(df_cm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#################visualizing confusion matrix"]}, {"cell_type": "markdown", "metadata": {}, "source": ["import some data to play with"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["iris = datasets.load_iris()\n", "X = iris.data\n", "y = iris.target\n", "class_names = iris.target_names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split the data into a training set and a test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run classifier, using a model that is too regularized (C too low) to see<br>\n", "the impact on the results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.set_printoptions(precision=2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot non-normalized confusion matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["titles_options = [(\"Confusion matrix, without normalization\", None),\n", "                  (\"Normalized confusion matrix\", 'true')]\n", "for title, normalize in titles_options:\n", "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n", "                                 display_labels=class_names,\n", "                                 cmap=plt.cm.Blues,\n", "                                 normalize=normalize)\n", "    disp.ax_.set_title(title)\n", "    print(title)\n", "    print(disp.confusion_matrix)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}