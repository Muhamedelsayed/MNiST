{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from utils.Convolution_util   import zero_pad, conv_forward, conv_backward, conv_SDLM\n", "from utils.Pooling_util       import pool_forward, pool_backward, subsampling_forward, subsampling_backward\n", "from utils.Activation_util    import activation_func\n", "from utils.RBF_initial_weight import rbf_init_weight\n", "from utils.utils_func         import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ConvLayer(object):\n", "    def __init__(self, kernel_shape, hparameters, init_mode='Gaussian_dist'):\n", "        \"\"\"\n", "        kernel_shape: (n_f, n_f, n_C_prev, n_C)\n", "        hparameters = {\"stride\": s, \"pad\": p}\n", "        \"\"\"\n", "        self.hparameters = hparameters\n", "        self.weight, self.bias = initialize(kernel_shape, init_mode)\n", "        self.v_w, self.v_b = np.zeros(kernel_shape), np.zeros((1,1,1,kernel_shape[-1]))\n", "        \n", "                #gyeen mn file convolution_util\n", "    def foward_prop(self, input_map): \n", "        output_map, self.cache = conv_forward(input_map, self.weight, self.bias, self.hparameters)\n", "        return output_map\n", "    \n", "    def back_prop(self, dZ, momentum, weight_decay):\n", "        dA_prev, dW, db = conv_backward(dZ, self.cache)\n", "        self.weight, self.bias, self.v_w, self.v_b = \\\n", "            update(self.weight, self.bias, dW, db, self.v_w, self.v_b, self.lr, momentum, weight_decay)\n", "        return dA_prev  \n", "    \n", "    def SDLM(self, d2Z, mu, lr_global):\n", "        d2A_prev, d2W = conv_SDLM(d2Z, self.cache)\n", "        h = np.sum(d2W)/d2Z.shape[0]\n", "        self.lr = lr_global / (mu + h)\n", "        return d2A_prev  \n", "    \n", "# C3: Convlayer with assigned combination between input maps and weight\n", "class ConvLayer_maps(object):\n", "    def __init__(self, kernel_shape, hparameters, mapping, init_mode='Gaussian_dist'):\n", "        \"\"\"\n", "        kernel_shape: (n_f, n_f, n_C_prev, n_C)\n", "        hparameters = {\"stride\": s, \"pad\": p}\n", "        \"\"\"\n", "        self.hparameters = hparameters\n", "        self.mapping     = mapping\n", "        self.wb   = []      # list of [weight, bias]\n", "        self.v_wb = []      # list of [v_w,    v_b]\n", "        for i in range(len(self.mapping)):\n", "            weight_shape = (kernel_shape[0], kernel_shape[1], len(self.mapping[i]), 1)\n", "            w, b = initialize(weight_shape, init_mode)\n", "            self.wb.append([w, b])\n", "            self.v_wb.append([np.zeros(w.shape), np.zeros(b.shape)])\n", "        \n", "    def foward_prop(self, input_map):\n", "        self.iputmap_shape = input_map.shape #(n_m,14,14,6)\n", "        self.caches = []\n", "        output_maps = []\n", "        for i in range(len(self.mapping)):\n", "            output_map, cache = conv_forward(input_map[:,:,:,self.mapping[i]], self.wb[i][0], self.wb[i][1], self.hparameters)\n", "            output_maps.append(output_map)\n", "            self.caches.append(cache)\n", "        output_maps = np.swapaxes(np.array(output_maps),0,4)[0]\n", "        return output_maps\n", "    \n", "    def back_prop(self, dZ, momentum, weight_decay):\n", "        dA_prevs = np.zeros(self.iputmap_shape)\n", "        for i in range(len(self.mapping)):\n", "            dA_prev, dW, db = conv_backward(dZ[:,:,:,i:i+1], self.caches[i])\n", "            self.wb[i][0], self.wb[i][1], self.v_wb[i][0], self.v_wb[i][1] =\\\n", "                update(self.wb[i][0], self.wb[i][1], dW, db, self.v_wb[i][0], self.v_wb[i][1], self.lr, momentum, weight_decay)\n", "            dA_prevs[:,:,:,self.mapping[i]] += dA_prev\n", "        return dA_prevs \n", "    \n", "    # Stochastic Diagonal Levenberg-Marquaedt\n", "    def SDLM(self, d2Z, mu, lr_global):\n", "        h = 0\n", "        d2A_prevs = np.zeros(self.iputmap_shape)\n", "        for i in range(len(self.mapping)):\n", "            d2A_prev, d2W = conv_SDLM(d2Z[:,:,:,i:i+1], self.caches[i])\n", "            d2A_prevs[:,:,:,self.mapping[i]] += d2A_prev\n", "            h += np.sum(d2W)\n", "        self.lr = lr_global / (mu + h/d2Z.shape[0])\n", "        return d2A_prevs "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PoolingLayer(object):\n", "    def __init__(self, hparameters, mode):\n", "        self.hparameters = hparameters\n", "        self.mode = mode\n", "        \n", "    def foward_prop(self, input_map):   # n,28,28,6 / n,10,10,16\n", "        A, self.cache = pool_forward(input_map, self.hparameters, self.mode)\n", "        return A\n", "    \n", "    def back_prop(self, dA):\n", "        dA_prev = pool_backward(dA, self.cache, self.mode)\n", "        return dA_prev\n", "    \n", "    def SDLM(self, d2A):\n", "        d2A_prev = pool_backward(d2A, self.cache, self.mode)\n", "        return d2A_prev"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Subsampling(object):\n", "    def __init__(self, n_kernel, hparameters):\n", "        self.hparameters = hparameters\n", "        self.weight = np.random.normal(0, 0.1, (1,1,1,n_kernel)) \n", "        self.bias   = np.random.normal(0, 0.1, (1,1,1,n_kernel)) \n", "        self.v_w = np.zeros(self.weight.shape)\n", "        self.v_b = np.zeros(self.bias.shape)\n", "        \n", "    def foward_prop(self, input_map):   # n,28,28,6 / n,10,10,16\n", "        A, self.cache = subsampling_forward(input_map, self.weight, self.bias, self.hparameters)\n", "        return A\n", "    \n", "    def back_prop(self, dA, momentum, weight_decay):\n", "        dA_prev, dW, db = subsampling_backward(dA, A_, weight, b, self.cache)\n", "        self.weight, self.bias, self.v_w, self.v_b = \\\n", "            update(self.weight, self.bias, dW, db, self.v_w, self.v_b, self.lr, momentum, weight_decay)\n", "        return dA_prev\n", "    \n", "    # Stochastic Diagonal Levenberg-Marquaedt\n", "    def SDLM(self, d2A, mu, lr_global):\n", "        d2A_prev, d2W, _ = subsampling_backward(dA, A_, weight, b, self.cache)\n", "        h = np.sum(d2W)/d2A.shape[0]\n", "        self.lr = lr_global / (mu + h)\n", "        return d2A_prev"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Activation(object):\n", "    def __init__(self, mode):    \n", "        (act, d_act), actfName = activation_func()\n", "        act_index  = actfName.index(mode)\n", "        self.act   = act[act_index]\n", "        self.d_act = d_act[act_index]\n", "        \n", "    def foward_prop(self, input_image): \n", "        self.input_image = input_image\n", "        return self.act(input_image)\n", "    \n", "    def back_prop(self, dZ):\n", "        dA = np.multiply(dZ, self.d_act(self.input_image)) \n", "        return dA\n", "    \n", "    # Stochastic Diagonal Levenberg-Marquaedt\n", "    def SDLM(self, d2Z):  #d2_LeNet5_squash\n", "        dA = np.multiply(d2Z, np.power(self.d_act(self.input_image),2)) \n", "        return dA"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class FCLayer(object):\n", "    def __init__(self, weight_shape, init_mode='Gaussian_dist'): \n", "        \n", "        # Initialization\n", "        self.v_w, self.v_b = np.zeros(weight_shape), np.zeros((weight_shape[-1],))\n", "        self.weight, self.bias = initialize(weight_shape, init_mode)\n", "        \n", "    def foward_prop(self, input_array):\n", "        self.input_array = input_array  #(n_m, 120)\n", "        return np.matmul(self.input_array, self.weight) # (n_m, 84)\n", "        \n", "    def back_prop(self, dZ, momentum, weight_decay):\n", "        dA = np.matmul(dZ, self.weight.T)               # (n_m, 84) * (84, 120) = (n_m, 120)\n", "        dW = np.matmul(self.input_array.T, dZ)          # (n_m, 120).T * (n_m, 84) = (120, 84)\n", "        db = np.sum(dZ.T, axis=1)                       # (84,)\n", "        \n", "        self.weight, self.bias, self.v_w, self.v_b = \\\n", "            update(self.weight, self.bias, dW, db, self.v_w, self.v_b, self.lr, momentum, weight_decay)\n", "        return dA\n", "    \n", "    # Stochastic Diagonal Levenberg-Marquaedt\n", "    def SDLM(self, d2Z, mu, lr_global):\n", "        d2A = np.matmul(d2Z, np.power(self.weight.T,2))\n", "        d2W = np.matmul(np.power(self.input_array.T,2), d2Z)\n", "        h = np.sum(d2W)/d2Z.shape[0]\n", "        self.lr = lr_global / (mu + h)\n", "        return d2A\n", "    \n", "# not even slightly work\n", "class RBFLayer_trainable_weight(object):\n", "    def __init__(self, weight_shape, init_weight=None, init_mode='Gaussian_dist'): \n", "        self.weight_shape = weight_shape # =(10, 84)\n", "        self.v_w = np.zeros(weight_shape)\n", "        if init_weight.shape == (10,84):\n", "            self.weight = init_weight\n", "        else:\n", "            self.weight, _ = initialize(weight_shape, init_mode)\n", "        \n", "    def foward_prop(self, input_array, label, mode): \n", "        \"\"\"\n", "        input_array = (n_m, 84)\n", "        label = (n_m, )\n", "        \"\"\"\n", "        \n", "        if mode == 'train':\n", "            self.input_array = input_array\n", "            self.weight_label = self.weight[label,:]  #(n_m, 84) labeled version of weight\n", "            loss = 0.5 * np.sum(np.power(input_array - self.weight_label, 2), axis=1, keepdims=True)  #(n_m, )\n", "            return np.sum(np.squeeze(loss))\n", "        \n", "        if mode == 'test':\n", "            subtract_weight = (input_array[:,np.newaxis,:] - np.array([self.weight]*input_array.shape[0])) # (n_m,10,84)\n", "            rbf_class = np.sum(np.power(subtract_weight,2), axis=2) # (n_m, 10)\n", "            class_pred = np.argmin(rbf_class, axis=1) # (n_m,)\n", "            error01 = np.sum(label != class_pred)\n", "            return error01, class_pred\n", "    def back_prop(self, label, lr, momentum, weight_decay):\n", "        #n_m = label.shape[0]\n", "        \n", "        #d_output = np.zeros((n_m, n_class))\n", "        #d_output[range(n_m), label] = 1    # (n_m, 10)  one-hot version of gradient w.r.t. output\n", "        \n", "        dy_predict = -self.weight_label + self.input_array    #(n_m, 84)\n", "        \n", "        dW_target  = -dy_predict                              #(n_m, 84)\n", "        \n", "        dW = np.zeros(self.weight_shape) # (10,84)\n", "        \n", "        for i in range(len(label)):  \n", "            dW[label[i],:] += dW_target[i,:]\n", "            \n", "        self.v_w = momentum*self.v_w - weight_decay*lr*self.weight - lr*dW\n", "        self.weight += self.v_w\n", "        return dy_predict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bitmap = rbf_init_weight()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RBFLayer(object):\n", "    def __init__(self, weight):        \n", "        self.weight = weight  # (10, 84)\n", "        \n", "    def foward_prop(self, input_array, label, mode): \n", "        \"\"\"\n", "        input_array = (n_m, 84)\n", "        label = (n_m, )\n", "        \"\"\"\n", "        if mode == 'train':\n", "            self.input_array = input_array\n", "            self.weight_label = self.weight[label,:]  #(n_m, 84) labeled version of weight\n", "            loss = 0.5 * np.sum(np.power(input_array - self.weight_label, 2), axis=1, keepdims=True)  #(n_m, )\n", "            return np.sum(np.squeeze(loss))\n", "        if mode == 'test':\n", "            # (n_m,1,84) - n_m*[(10,84)] = (n_m,10,84)\n", "            subtract_weight = (input_array[:,np.newaxis,:] - np.array([self.weight]*input_array.shape[0])) # (n_m,10,84)\n", "            rbf_class = np.sum(np.power(subtract_weight,2), axis=2) # (n_m, 10)\n", "            class_pred = np.argmin(rbf_class, axis=1) # (n_m,)\n", "            error01 = np.sum(label != class_pred)\n", "            return error01, class_pred\n", "        \n", "    def back_prop(self):\n", "        dy_predict = -self.weight_label + self.input_array    #(n_m, 84)\n", "        return dy_predict\n", "    \n", "    def SDLM(self):\n", "        # d2y_predict\n", "        return np.ones(self.input_array.shape)\n", "    \n", "    \n", "class LeNet5(object):\n", "    def __init__(self):\n", "        kernel_shape = {\"C1\": (5,5,1,6),\n", "                        \"C3\": (5,5,6,16),    ### C3 has designated combinations\n", "                        \"C5\": (5,5,16,120),  ### It's actually a FC layer\n", "                        \"F6\": (120,84),\n", "                        \"OUTPUT\": (84,10)}\n", "        \n", "        hparameters_convlayer = {\"stride\": 1, \"pad\": 0}\n", "        hparameters_pooling   = {\"stride\": 2, \"f\": 2}        \n", "        \n", "        self.C1 = ConvLayer(kernel_shape[\"C1\"], hparameters_convlayer)\n", "        self.a1 = Activation(\"LeNet5_squash\")\n", "        self.S2 = PoolingLayer(hparameters_pooling, \"average\")\n", "        \n", "        self.C3 = ConvLayer_maps(kernel_shape[\"C3\"], hparameters_convlayer, C3_mapping)\n", "        self.a2 = Activation(\"LeNet5_squash\")\n", "        self.S4 = PoolingLayer(hparameters_pooling, \"average\")\n", "        \n", "        self.C5 = ConvLayer(kernel_shape[\"C5\"], hparameters_convlayer)\n", "        self.a3 = Activation(\"LeNet5_squash\")\n", "        self.F6 = FCLayer(kernel_shape[\"F6\"])\n", "        self.a4 = Activation(\"LeNet5_squash\")\n", "        \n", "        #self.Output = RBFLayer(kernel_shape[\"OUTPUT\"], bitmap)\n", "        self.Output = RBFLayer(bitmap)\n", "        \n", "    def Forward_Propagation(self, input_image, input_label, mode): \n", "        self.label = input_label\n", "        self.C1_FP = self.C1.foward_prop(input_image)\n", "        self.a1_FP = self.a1.foward_prop(self.C1_FP)\n", "        self.S2_FP = self.S2.foward_prop(self.a1_FP)\n", "        self.C3_FP = self.C3.foward_prop(self.S2_FP)\n", "        self.a2_FP = self.a2.foward_prop(self.C3_FP)\n", "        self.S4_FP = self.S4.foward_prop(self.a2_FP)\n", "        self.C5_FP = self.C5.foward_prop(self.S4_FP)\n", "        self.a3_FP = self.a3.foward_prop(self.C5_FP)\n", "        self.flatten = self.a3_FP[:,0,0,:]\n", "        self.F6_FP = self.F6.foward_prop(self.flatten)\n", "        self.a4_FP = self.a4.foward_prop(self.F6_FP)  \n", "        \n", "        # output sum of the loss over mini-batch when mode = 'train'\n", "        # output class when mode = 'test'\n", "        out  = self.Output.foward_prop(self.a4_FP, input_label, mode) \n", "        return out \n", "        \n", "    def Back_Propagation(self, momentum, weight_decay):\n", "        dy_pred = self.Output.back_prop()\n", "        \n", "        dy_pred = self.a4.back_prop(dy_pred)\n", "        F6_BP = self.F6.back_prop(dy_pred, momentum, weight_decay)\n", "        reverse_flatten = F6_BP[:,np.newaxis,np.newaxis,:]\n", "        \n", "        reverse_flatten = self.a3.back_prop(reverse_flatten) \n", "        C5_BP = self.C5.back_prop(reverse_flatten, momentum, weight_decay)\n", "        \n", "        S4_BP = self.S4.back_prop(C5_BP)\n", "        S4_BP = self.a2.back_prop(S4_BP)\n", "        C3_BP = self.C3.back_prop(S4_BP, momentum, weight_decay) \n", "        \n", "        S2_BP = self.S2.back_prop(C3_BP)\n", "        S2_BP = self.a1.back_prop(S2_BP)  \n", "        C1_BP = self.C1.back_prop(S2_BP, momentum, weight_decay)\n", "        \n", "    # Stochastic Diagonal Levenberg-Marquaedt method for determining the learning rate \n", "    def SDLM(self, mu, lr_global):\n", "        d2y_pred = self.Output.SDLM()\n", "        d2y_pred = self.a4.SDLM(d2y_pred)\n", "        \n", "        F6_SDLM = self.F6.SDLM(d2y_pred, mu, lr_global)\n", "        reverse_flatten = F6_SDLM[:,np.newaxis,np.newaxis,:]\n", "        \n", "        reverse_flatten = self.a3.SDLM(reverse_flatten) \n", "        C5_SDLM = self.C5.SDLM(reverse_flatten, mu, lr_global)\n", "        \n", "        S4_SDLM = self.S4.SDLM(C5_SDLM)\n", "        S4_SDLM = self.a2.SDLM(S4_SDLM)\n", "        C3_SDLM = self.C3.SDLM(S4_SDLM, mu, lr_global)\n", "        \n", "        S2_SDLM = self.S2.SDLM(C3_SDLM)\n", "        S2_SDLM = self.a1.SDLM(S2_SDLM)  \n", "        C1_SDLM = self.C1.SDLM(S2_SDLM, mu, lr_global)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}
